---
title: 'Project Proposal: Happiness Report’s top tier for multiple consecutive years'
author: "Amal Dagan, Bar Evron, Niv Dolev, Roni London"
output:
  html_document:
    df_print: paged
  pdf_document:
    number_sections: true
    fig_caption: true
    latex_engine: xelatex
---

\newpage

```{r, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```

```{r libreries}
library(tidyverse)
library(dplyr)
library(stringr)
library(ggplot2)
library(patchwork)
library(caret)
library(pROC)
library(rpart.plot)
library(readxl)  # a libary for loading excel files
```

# Introduction

**Research Question**\
Which indicators enable a country to remain in the World Happiness Report’s top tier for multiple consecutive years?

**Background**\
GDP per capita, social support, life expectancy and perceived freedom are repeatedly cited as key drivers of well-being.

**Current Gap and existing research**\
Several studies have examined national happiness using WHR data, though few have explored long-term patterns. For example, Kuppens et al. (2023) analyzed cultural factors in 78 countries before and after the COVID-19 pandemic. They found that individualism and indulgence became stronger predictors of happiness post-COVID, suggesting that global crises may shift the importance of cultural values. However, their analysis compared only two time points (2017–2019 vs. 2021) and did not assess whether these associations hold across a longer period. Similarly, Al‐Maatouq and Al Shammari (2022) examined the relationship between WHR scores, Hofstede’s cultural dimensions, and government education spending across 58 countries. They found that countries with higher long-term orientation and lower power distance tended to score higher on the WHR, indicating possible structural links between culture and well-being. However, their study was cross-sectional and did not account for how such associations might change over time. Despite the value of these contributions, our research takes a unique direction. We combine an unusually broad dataset—spanning 2011 to 2024 and covering over 150 countries—with repeated-measures modeling to assess which happiness predictors remain robust over time. Unlike studies that focus on a single year or a narrow method, our approach allows us to explore both consistency and change in global well-being, providing updated and policy-relevant insights.

**Contribution**\
Turning the dataset’s continual renewal from hurdle to asset, we deliver up-to-date policy guidance and reveal whether the perennial “Top-20” countries stay happy for the same reasons or for shifting, time-dependent ones.

# Data

We use publicly available data from the World Happiness Report website, based on the Gallup World Poll, covering the years 2005 to 2024. The dataset contains 2,283 yearly observations across approximately 100 to 130 countries per year, with 14 variables per country-year entry. Each observation represents a single country in a given year. The variables include social, health, and economic factors widely studied in happiness research, such as: social support, GDP per capita, healthy life expectancy, freedom to make life choices, generosity, and perceptions of corruption, as well as benevolance factors like donations, volunteering and helping a stranger. Our binary outcome variable indicates whether a country was ranked in the Top 20 happiest countries that year. 

A detailed data dictionary is included in the /data/README.md file in our repository.

# Preliminary Results

```{r loading data}
happiness_data <- read.csv("data/merged_whr_data.csv")
happiness_data <- happiness_data %>% 
  filter(!is.na(Average..annual.)) 

null_countries <- c("China", "Kosovo", "South Sudan", "State of Palestine", "Turkmenistan", "United Arab Emirates", "Saudi Arabia", "Kuwait", "Bahrain", "Jordan", "Côte d’Ivoire", "Hong Kong SAR of China", "Netherlands", "Taiwan Province of China", "Cuba", "Maldives", "Oman", "Sudan")

better_data = subset(happiness_data, !(Country %in% null_countries))

happiness_data <- better_data %>%
  group_by(Country) %>%
  mutate(across(
    where(is.numeric),
    ~ ifelse(is.na(.), mean(., na.rm = TRUE), .)
  )) %>%
  ungroup()
```

```{r plots-and-table, fig.width=9, fig.height=5, echo=FALSE, warning=FALSE, message=FALSE}

happiness_data <- happiness_data %>%
  group_by(Year) %>%
  mutate(Top_20 = Average..annual..rank <= 20) %>%# create a new top_20 column
  mutate(Top_15 = Average..annual..rank <= 15) %>%# create a new top_15 column
  mutate(Top_25 = Average..annual..rank <= 25) %>%# create a new top_25 column
  
  ungroup() %>%
  #renaming the column in order to be usable in R
  rename(Country_name = Country, Ladder_score = Average..annual., GDP = GDP.per.capita, Social_support = Social.support, Life_expectancy = Healthy.life.expectancy, Corruption = Perceptions.of.corruption, Rank = Average..annual..rank, Pos_emotions = Positive.emotions, Neg_emotions = Negative.emotions, Helped_stranger = Helped.a.stranger)
happiness_data <- happiness_data %>%
  arrange(Country_name, Year) %>%  # Sort by country and year
  group_by(Country_name) %>%      # Process each country separately
  mutate(Top20_Last = lag(Top_20)) %>%  # Get last year's Top_20 and create new column
  ungroup() %>%  # Remove grouping
  mutate(Top20_Last = case_when(
    is.na(Top20_Last) ~ 2,         # No previous data
    Top20_Last == 1 ~ 1,           # Was in top 20
    Top20_Last == 0 ~ 0            # Was not in top 20
  ))
happiness_data$Log_GDP <- log(happiness_data$GDP)

countries_per_year <- happiness_data %>%
  group_by(Year) %>%
  summarise(
    Num_Countries = n_distinct(Country_name)
  ) %>%
  arrange(Year)

summary(happiness_data)

country_summary <- happiness_data %>%
  group_by(Country_name) %>%
  summarise(
    Years_in_DB = n(), # Number of records (years) per country
    Avg_Rank = round(mean(Rank, na.rm = TRUE), 2), # Average rank
    Top_20_Count = sum(Top_20, na.rm = TRUE), # Number of times Top_20 is TRUE
    Top_20_Ratio = round(Top_20_Count / Years_in_DB, 2) 
  ) %>%
  arrange(desc(Top_20_Ratio)) # Optional: sort by best rank

# count Top_20 appearences by country
top_countries_long <- happiness_data %>%
  filter(Top_20) %>%
  count(Country_name, name = "Years_in_Top_20") %>%
  arrange(desc(Years_in_Top_20))

tpl1 <- country_summary %>%
  filter(Top_20_Ratio>0.3) %>%
  ggplot(aes(x = reorder(Country_name, Top_20_Ratio), y = Top_20_Ratio)) +
  geom_col(fill = "lightgreen") +
  coord_flip() +
  labs(
    title = "Percentage of Years Each Country was in the Top 20",
    x = "Country",
    y = "Ratio of years"
  ) +
  theme_minimal(base_size = 13) +
  theme(
    plot.title = element_text(size = 12, face = "bold"),
    axis.text = element_text(size = 12),
    axis.title = element_text(size = 13)
  )

plt1 <- ggplot(happiness_data, aes(x = Social_support, y = Ladder_score)) +
  geom_point(alpha=0.4) +
  geom_smooth(method="lm", se=FALSE, color="blue") +
  labs(title="Ladder score vs Social support")

plt <- ggplot(happiness_data, aes(x=Rank)) + geom_histogram(color="darkblue", fill="lightblue", binwidth=3) +
  labs(title="Distribution of rankings")+
  theme_minimal(base_size = 13) +
  theme(
    plot.title = element_text(size = 12, face = "bold"),
    axis.text = element_text(size = 12),
    axis.title = element_text(size = 13)
  )
```

```{r top_countries_long, fig.width=10, fig.height=5, echo=FALSE, warning=FALSE, message=FALSE}
tpl1 | plt
```

The distribution of the percentage of years in the top 20 is kept pretty limited, only 11 countries have never left the top 20 spots in their reported time. This goes to show that being a top country is a perpetual title. Which means there is merit to asking this question, we think there is a pattern of data that indicates how consistent a country is at staying at the top of the list.

# Work Plan

The outcome is the Top-20 column and the predictors currently are all of the columns. The way we will answer the question is that we will attempt to find the most contributing variables in order to answer the question.

we will use a two different methods to answer the question: a decision-tree and a logistic regression model. we will attempt to build those models concurrently and to compare and contrast them.

we would like to receive some conclusive results (the p_value of some of the variables will be significant), in all models so after comparison we could isolate the most relevant factors.

-   Data preparation and cleaning. (Bar)
-   Exploratory analysis and visualization. (Roni)
-   Model building and evaluation. (Roni,Niv)
-   Interpretation of results and final reporting. (Amal,Niv)

\\\

**models and fun** The main model: Decision Tree

Like logistic regression this is also a popular model for classification problems, and since it can be visualized it is an obvious choice to be one of the models.

The second model example: Logistic Regression

If we're to take a classification route, then logistic regression would be a good model for that. We got good results, but again maybe too good. All of the predictors seem to be strong predictors which was good for the sake of showing that there is a correlation, but the performance was too accurate to use as the main model.

*decision tree models*

we will create 2 sets of 3 diffrent tree models
the first set will include the following models: 
the first model will be a decision tree predicting the top_15 parameter and will use the chronologically split data.
the second model will be a decision tree predicting the top_20 parameter and will use the chronologically split data. 
the third model will be a decision tree predicting the top_25 parameter and will use the chronologically split data. 

the second set will include the following models: 
the first model will be a decision tree predicting the top_15 parameter and will use the randomly split data.
the second model will be a decision tree predicting the top_20 parameter and will use the randomly split data. 
the third model will be a decision tree predicting the top_25 parameter and will use the randomly split data. 

we start with chronologically spliting the data for prior and post 2022 in an almost 80-20 split

```{r first_data_split}

# Split data
train_data_chron <- subset(happiness_data, Year < 2022)
test_data_chron <- subset(happiness_data, Year >= 2022)

```
**using 11 parameters**

*first set*

*first model*

the first model training and pie chart of the factor importance

```{r first_model_training 12 parameters}
first_tree_model_12 <- rpart(
  Top_15 ~ Log_GDP + Inequality + Social_support + Life_expectancy + Pos_emotions +
    Freedom + Generosity + Corruption + Donated + Volunteered +Helped_stranger+Neg_emotions,
  data = train_data_chron,
  method = "class"
)
rpart.plot(first_tree_model_12)# the tree

importance_1_12 <- first_tree_model_12$variable.importance# variable importance scores

pie(importance_1_12,
    main = "Variable Importance (Decision Tree)",
    col = rainbow(length(importance_1_12)), cex=0.8)
# variable importance pie chart for better visual dislay of importance
```

the first tree model predictions had a 93% accuracy which shows that the model predicts if a country is in the top 15 well. but it is too accurate, probably because of an abundance of factors. The split in the confusion matrix looks good, so we continue testing.

```{r first_model_predict 12 parameters}

test_data_chron$Top_15 <- as.factor(test_data_chron$Top_15)
# Predict class labels (Top_20) for test_data

predictions <- predict(first_tree_model_12, newdata = test_data_chron, type = "class")
# Confusion matrix

conf_matrix <- table(Predicted = predictions, Actual = test_data_chron$Top_15)
print(conf_matrix)

# Accuracy
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
cat("Test Accuracy:", round(accuracy, 4), "\n")

```

*second model*
the second model training and pie chart of the factor importance

```{r second_model_training 12 parameters}

# Train the decision tree using rpart
second_tree_model_12 <- rpart(Top_20 ~ Log_GDP + Inequality + Social_support + Life_expectancy + Pos_emotions + Freedom + Generosity + Corruption + Donated + Volunteered +Helped_stranger +Neg_emotions ,data = train_data_chron, method = "class")
rpart.plot(second_tree_model_12)

importance_2_12 <- second_tree_model_12$variable.importance

pie(importance_2_12,
    main = "Variable Importance (Decision Tree)",
    col = rainbow(length(importance_2_12)), cex=0.8)
# variable importance pie chart for better visual dislay of importance
```

The second model is similar to the first, and predicts the top 20 countries and not top 15. The accuracy this time is even higher, at almost 95%.
The factors are all the same, it may be a sign that predicting the top 15 is better than the top 20.

```{r second_model_predict 12 parameters}

test_data_chron$Top_20 <- as.factor(test_data_chron$Top_20)
# Predict class labels (Top_20) for test_data
predictions <- predict(second_tree_model_12, newdata = test_data_chron, type = "class")
# Confusion matrix
conf_matrix <- table(Predicted = predictions, Actual = test_data_chron$Top_20)
print(conf_matrix)

# Accuracy
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
cat("Test Accuracy:", round(accuracy, 4), "\n")

```

*third model*

the third model training and pie chart of the factor importance best model

```{r third_model_training 12 parameters}

third_tree_model_12 <- rpart(Top_25 ~  Log_GDP + Inequality + Social_support + Life_expectancy + Pos_emotions + Freedom + Generosity + Corruption + Donated + Volunteered +Helped_stranger+Neg_emotions ,data = train_data_chron, method = "class")
rpart.plot(third_tree_model_12)

importance_3_12 <- third_tree_model_12$variable.importance

pie(importance_3_12,
    main = "Variable Importance (Decision Tree)",
    col = rainbow(length(importance_3_12)), cex=0.8)
# variable importance pie chart for better visual dislay of importance

```

the third tree model predictions had a 90% accuracy, which is much better than the previous models. we used all of the factors again but this time tried to predict the top 25.
so far this is the best out of all of the models.
```{r third_model_predict 12 parameters}

test_data_chron$Top_25 <- as.factor(test_data_chron$Top_25)
# Predict class labels (Top_20) for test_data
predictions <- predict(third_tree_model_12, newdata = test_data_chron, type = "class")
# Confusion matrix
conf_matrix <- table(Predicted = predictions, Actual = test_data_chron$Top_25)
print(conf_matrix)

# Accuracy
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
cat("Test Accuracy:", round(accuracy, 4), "\n")

```

The top 7 factors were: Log_GDP, Corruption, Social_support, Freedom, Pos_emotions, Life_expectancy, and Generosity.
```{r}
# Example data (replace with your actual vectors)
timevec <- (importance_1_12+importance_2_12+importance_3_12)
timevec <-timevec/sum(timevec) * 100
print(timevec)
```

we will then split the data randomly and check if the results differ.

```{r second-data_split}

happiness_data <- happiness_data[sample(nrow(happiness_data)), ]#take the happiness data
sample <- sample(c(TRUE, FALSE), nrow(happiness_data), replace=TRUE, prob=c(0.8,0.2))# split the data 80-20
train_data_rand  <- happiness_data[sample, ]
test_data_rand   <- happiness_data[!sample, ]

```
*second set*

*fourth model*

the fourth model training and pie chart of the factor importance

```{r fourth_model_training 12 parameters}

fourth_tree_model_12 <- rpart(
  Top_15 ~ Log_GDP + Inequality + Social_support + Life_expectancy + Pos_emotions + Freedom + Generosity + Corruption + Donated + Volunteered +Helped_stranger+Neg_emotions,data = train_data_rand,
  method = "class"
)
rpart.plot(fourth_tree_model_12)# the tree

importance_4 <- fourth_tree_model_12$variable.importance# variable importance scores
print(importance_4)

pie(importance_4,
    main = "Variable Importance (Decision Tree)",
    col = rainbow(length(importance_4)), cex=0.8)
# variable importance pie chart for better visual dislay of importance
```

the fourth tree model predictions had a 94% accuracy which is very slightly higher than 93% of the corresponding model. that might show that the split by time is beneficial for creating a more realistic model, but we need more testing to be sure.

```{r fourth_model_predict 12 parameters}

test_data_rand$Top_15 <- as.factor(test_data_rand$Top_15)
# Predict class labels (Top_20) for test_data

predictions <- predict(fourth_tree_model_12, newdata = test_data_rand, type = "class")
# Confusion matrix

conf_matrix <- table(Predicted = predictions, Actual = test_data_rand$Top_15)
print(conf_matrix)

# Accuracy
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
cat("Test Accuracy:", round(accuracy, 4), "\n")

```

*fifth model*
the fifth model training and pie chart of the factor importance

```{r fifth_model_training 12 parameters}

# Train the decision tree using rpart
fifth_tree_model_12 <- rpart(Top_20 ~  Log_GDP + Inequality + Social_support + Life_expectancy + Pos_emotions + Freedom + Generosity + Corruption + Donated + Volunteered +Helped_stranger+Neg_emotions ,data = train_data_rand, method = "class")
rpart.plot(fifth_tree_model_12)

importance_5 <- fifth_tree_model_12$variable.importance
print(importance_5)

pie(importance_5,
    main = "Variable Importance (Decision Tree)",
    col = rainbow(length(importance_5)), cex=0.8)
# variable importance pie chart for better visual dislay of importance
```

the fifth model this time shows a decrease in accuracy, but it's still high at 93%.

```{r fifth_model_predict 12 parameters}

test_data_rand$Top_20 <- as.factor(test_data_rand$Top_20)
# Predict class labels (Top_20) for test_data
predictions <- predict(fifth_tree_model_12, newdata = test_data_rand, type = "class")
# Confusion matrix
conf_matrix <- table(Predicted = predictions, Actual = test_data_rand$Top_20)
print(conf_matrix)

# Accuracy
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
cat("Test Accuracy:", round(accuracy, 4), "\n")

```

*sixth model*

the sixth model training and pie chart of the factor importance

```{r sixth_model_training 12 parameters}

sixth_tree_model_12 <- rpart(Top_25 ~  Log_GDP + Inequality + Social_support + Life_expectancy + Pos_emotions + Freedom + Generosity + Corruption + Donated + Volunteered + Helped_stranger+Neg_emotions ,data = train_data_rand, method = "class")
rpart.plot(sixth_tree_model_12)

importance_6 <- sixth_tree_model_12$variable.importance
print(importance_6)

pie(importance_6,
    main = "Variable Importance (Decision Tree)",
    col = rainbow(length(importance_6)), cex=0.8)
# variable importance pie chart for better visual dislay of importance

```

the sixth model predictions which shows a 93% accuracy which is noticeably higher than the corresponding accuracy of the timed model.
the best combination of features for the model so far has been predicting the top 25 countries with a timed split, at 90%.

```{r sixth_model_predict 12 parameters}

test_data_rand$Top_25 <- as.factor(test_data_rand$Top_25)
# Predict class labels (Top_20) for test_data
predictions <- predict(sixth_tree_model_12, newdata = test_data_rand, type = "class")
# Confusion matrix
conf_matrix <- table(Predicted = predictions, Actual = test_data_rand$Top_25)
print(conf_matrix)

# Accuracy
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
cat("Test Accuracy:", round(accuracy, 4), "\n")

```
Top 7 factors are: Log_GDP, Corruption, Freedom, Life_expectancy, Social_support, Pos_emotions, and Volunteered.
```{r average 12 parameter random model importance}
# Example data (replace with your actual vectors)
randvec <- (importance_4+importance_5+importance_6)
randvec <-randvec/sum(randvec) * 100
print(randvec)
```
Top 7 factors in both tree model sets:
Log_GDP, Corruption, Freedom, Life_expectancy, Social_support, Pos_emotions, and Volunteered.
```{r average 12 parameter random and chron model importance}
# Example data (replace with your actual vectors)
avg_vec<-(randvec+timevec)/2
print(sort(avg_vec,decreasing = TRUE))
```
**7 parameters**

new tree models based on 7 parameters

*first set*

*first model*

```{r first_model_training 7 parameters}
first_tree_model_7 <- rpart(
  Top_15 ~ Log_GDP + Social_support + Life_expectancy + Pos_emotions +
    Freedom + Corruption + Volunteered,
  data = train_data_chron,
  method = "class"
)
rpart.plot(first_tree_model_7)# the tree

importance_1_7 <- first_tree_model_7$variable.importance# variable importance scores

pie(importance_1_7,
    main = "Variable Importance (Decision Tree)",
    col = rainbow(length(importance_1_7)))
# variable importance pie chart for better visual dislay of importance
```

the first tree model predictions had a 93% accuracy, which is exactly like how it was having all factors.

```{r first_model_predict 7 parameters}

test_data_chron$Top_15 <- as.factor(test_data_chron$Top_15)
# Predict class labels (Top_20) for test_data

predictions <- predict(first_tree_model_7, newdata = test_data_chron, type = "class")
# Confusion matrix

conf_matrix <- table(Predicted = predictions, Actual = test_data_chron$Top_15)
print(conf_matrix)

# Accuracy
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
cat("Test Accuracy:", round(accuracy, 4), "\n")

```

*second model*

the second model training and pie chart of the factor importance

```{r second_model_training 7 parameters}

# Train the decision tree using rpart
second_tree_model_7 <- rpart(Top_20 ~ Log_GDP + Social_support + Life_expectancy + Pos_emotions +Freedom + Corruption + Volunteered ,data = train_data_chron, method = "class")
rpart.plot(second_tree_model_7)

importance_2_7 <- second_tree_model_7$variable.importance

pie(importance_2_7,
    main = "Variable Importance (Decision Tree)",
    col = rainbow(length(importance_2_7)))
# variable importance pie chart for better visual dislay of importance
```

the second model predictions had almost 95%, which is also like before.

```{r second_model_predict 7 parameters}

test_data_chron$Top_20 <- as.factor(test_data_chron$Top_20)
# Predict class labels (Top_20) for test_data
predictions <- predict(second_tree_model_7, newdata = test_data_chron, type = "class")
# Confusion matrix
conf_matrix <- table(Predicted = predictions, Actual = test_data_chron$Top_20)
print(conf_matrix)

# Accuracy
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
cat("Test Accuracy:", round(accuracy, 4), "\n")

```

*third model*

the third model training and pie chart of the factor importance best model

```{r third_model_training 7 parameters}

third_tree_model_7 <- rpart(Top_25 ~  Log_GDP + Social_support + Life_expectancy + Pos_emotions +Freedom + Corruption + Volunteered ,data = train_data_chron, method = "class")
rpart.plot(third_tree_model_7)

importance_3_7 <- third_tree_model_7$variable.importance

pie(importance_3_7,
    main = "Variable Importance (Decision Tree)",
    col = rainbow(length(importance_3_7)))
# variable importance pie chart for better visual dislay of importance

```

the third tree model also predicted with 90% accuracy like with the corresponding model, so there seems to be no change when using less factors. good or bad.

```{r third_model_predict 7 parameters}

test_data_chron$Top_25 <- as.factor(test_data_chron$Top_25)
# Predict class labels (Top_20) for test_data
predictions <- predict(third_tree_model_7, newdata = test_data_chron, type = "class")
# Confusion matrix
conf_matrix <- table(Predicted = predictions, Actual = test_data_chron$Top_25)
print(conf_matrix)

# Accuracy
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
cat("Test Accuracy:", round(accuracy, 4), "\n")

```
Top 7 factors are: Log_GDP,Corruption,Social_support,Freedom,Pos_emotions,Donated,Life_expectancy.  
.
```{r}
# Example data (replace with your actual vectors)
newtimevec <- (importance_1_7+importance_2_7+importance_3_7)
newtimevec <-newtimevec/sum(newtimevec) * 100
print(newtimevec)
```
*second set*

*fourth model*

the fourth model training and pie chart of the factor importance

```{r fourth_model_training 7 parameters}

fourth_tree_model_7 <- rpart(
  Top_15 ~ Log_GDP + Social_support + Life_expectancy + Pos_emotions +Freedom + Corruption + Volunteered,data = train_data_rand,method = "class"
)
rpart.plot(fourth_tree_model_7)# the tree

importance_4_7 <- fourth_tree_model_7$variable.importance# variable importance scores

pie(importance_4_7,
    main = "Variable Importance (Decision Tree)",
    col = rainbow(length(importance_4_7)))
# variable importance pie chart for better visual dislay of importance
```

the fourth model showed an accuracy of 93% similar to most other models, and also the same as the iteration with the full list of factors.

```{r fourth_model_predict 7 parameters}

test_data_rand$Top_15 <- as.factor(test_data_rand$Top_15)
# Predict class labels (Top_20) for test_data

predictions <- predict(fourth_tree_model_7, newdata = test_data_rand, type = "class")
# Confusion matrix

conf_matrix <- table(Predicted = predictions, Actual = test_data_rand$Top_15)
print(conf_matrix)

# Accuracy
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
cat("Test Accuracy:", round(accuracy, 4), "\n")

```

*fifth model*
the fifth model training and pie chart of the factor importance

```{r fifth_model_training 7 parameters}

# Train the decision tree using rpart
fifth_tree_model_7 <- rpart(Top_20 ~  Log_GDP + Social_support + Life_expectancy + Pos_emotions +Freedom + Corruption + Volunteered ,data = train_data_rand, method = "class")
rpart.plot(fifth_tree_model_7)

importance_5_7 <- fifth_tree_model_7$variable.importance


pie(importance_5_7,
    main = "Variable Importance (Decision Tree)",
    col = rainbow(length(importance_5_7)))
# variable importance pie chart for better visual dislay of importance
```

the fifth model seems to be doing better, with an accuracy of 93.5% instead of almost 95%.

```{r fifth_model_predict 7 parameters}

test_data_rand$Top_20 <- as.factor(test_data_rand$Top_20)
# Predict class labels (Top_20) for test_data
predictions <- predict(fifth_tree_model_7, newdata = test_data_rand, type = "class")
# Confusion matrix
conf_matrix <- table(Predicted = predictions, Actual = test_data_rand$Top_20)
print(conf_matrix)

# Accuracy
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
cat("Test Accuracy:", round(accuracy, 4), "\n")

```

*sixth model*

the sixth model training and pie chart of the factor importance

```{r sixth_model_training 7 parameters}

sixth_tree_model_7 <- rpart(Top_25 ~  Log_GDP + Social_support + Life_expectancy + Pos_emotions +Freedom + Corruption + Volunteered ,data = train_data_rand, method = "class")
rpart.plot(sixth_tree_model_7)

importance_6_7 <- sixth_tree_model_7$variable.importance


pie(importance_6_7,
    main = "Variable Importance (Decision Tree)",
    col = rainbow(length(importance_6_7)))
# variable importance pie chart for better visual dislay of importance

```

the sixth model surprisingly performed slightly worse, with a high accuracy of 91.5%, seeing as the past 90% accuracy scores are more favorable. 

```{r sixth_model_predict 7 parameters}

test_data_rand$Top_25 <- as.factor(test_data_rand$Top_25)
# Predict class labels (Top_20) for test_data
predictions <- predict(sixth_tree_model_7, newdata = test_data_rand, type = "class")
# Confusion matrix
conf_matrix <- table(Predicted = predictions, Actual = test_data_rand$Top_25)
print(conf_matrix)

# Accuracy
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
cat("Test Accuracy:", round(accuracy, 4), "\n")

```
```{r}
# Example data (replace with your actual vectors)
newrandvec <- (importance_4_7+importance_5_7+importance_6_7)
newrandvec <-newrandvec/sum(newrandvec) * 100
print(newrandvec)
```
Top 7 factors in tree models:
```{r}
# Example data (replace with your actual vectors)
new_avg_vec<-(newrandvec+newtimevec)/2
new_avg_vec<-sort(new_avg_vec,decreasing = TRUE)
print(new_avg_vec)
```

*logistic regression models*

in this section we created two logistic regression models which differ in the test and train data they recieve.

*first logistic regression model* the first model uses a randomly split data and tries to use logistic regression to determine whether a country is in the top 20. this model had an accuracy of 95%, which is way worse than our best performing model so far.
```{r  first logistic regression model 12 parameters , echo=FALSE, warning=FALSE, message=FALSE}
set.seed(123)

# Split data into train/test
happiness_data <- happiness_data[sample(nrow(happiness_data)), ]
sample <- sample(c(TRUE, FALSE), nrow(happiness_data), replace = TRUE, prob = c(0.8, 0.2))
train <- happiness_data[sample, ]
test <- happiness_data[!sample, ]

# Predictor columns
cols <- c("Log_GDP", "Social_support", "Life_expectancy", "Freedom", "Generosity", "Corruption", "Inequality", "Pos_emotions", "Neg_emotions", "Donated", "Volunteered", "Helped_stranger")

# Build formula
predictors <- paste(cols, collapse = " + ")
log_formula <- as.formula(paste("Top_20 ~", predictors))

# Fit logistic model
logmodel <- glm(log_formula, data = train, family = "binomial")

# Predict on test set
probs <- predict(logmodel, newdata = test, type = "response")
preds <- ifelse(probs > 0.5, 1, 0)

# Evaluate
test$Top_20 <- as.integer(test$Top_20 == TRUE)
test$preds <- preds

library(caret)
cm <- confusionMatrix(factor(test$preds, levels = c(0, 1)),
                      factor(test$Top_20, levels = c(0, 1)))

# Output results
summary(logmodel)
print(cm)

# Variable importance
library(caret)
imps2 <- varImp(logmodel) %>% arrange(desc(Overall))
print(imps2)
# Convert to named numeric vector
imps_vector <- as.numeric(imps2$Overall)
names(imps_vector) <- rownames(imps2)
pie(imps_vector, main = "Variable Importance (Logistic regression)",
    col = rainbow(length(imps_vector)), cex=0.8)

importance_3_12 <- importance_3_12[names(importance_3_12) != "Donated"]
ref <- importance_3_12["Log_GDP"]
ref2 <- imps_vector["Pos_emotions"]
ratio <- ref2 / ref
imps_vector_normalized <- imps_vector / ratio

logistic_df <- enframe(imps_vector_normalized, name = "Variable", value = "Logistic_Normalized")
tree_df <- enframe(importance_3_12, name = "Variable", value = "Tree")
combined_df <- full_join(logistic_df, tree_df, by = "Variable")

long_df <- pivot_longer(combined_df, 
                        cols = c(Logistic_Normalized, Tree), 
                        names_to = "Model", 
                        values_to = "Importance")

long_df$Variable <- factor(long_df$Variable, 
                           levels = combined_df %>%
                             arrange(desc(Logistic_Normalized)) %>%
                             pull(Variable))

ggplot(long_df, aes(x = Variable, y = Importance, fill = Model)) +
  geom_bar(stat = "identity", position = "dodge") +
  scale_fill_manual(values = c("steelblue", "salmon")) +
  labs(title = "Comparison of Variable Importances (Normalized by the most valuable factor)",
       y = "Importance (Relative Scale)",
       x = NULL) +
  theme_minimal(base_size = 10) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        plot.title = element_text(size = 10))
```
7 factor random logistic regression model
```{r  first logistic regression model 7 parameters , echo=FALSE, warning=FALSE, message=FALSE}
set.seed(123)

# Split data into train/test
happiness_data <- happiness_data[sample(nrow(happiness_data)), ]
sample <- sample(c(TRUE, FALSE), nrow(happiness_data), replace = TRUE, prob = c(0.8, 0.2))
train <- happiness_data[sample, ]
test <- happiness_data[!sample, ]


# Predictor columns
cols <- c("Log_GDP", "Social_support", "Life_expectancy","Corruption","Inequality", "Pos_emotions", "Helped_stranger")

# Build formula
predictors <- paste(cols, collapse = " + ")
log_formula <- as.formula(paste("Top_20 ~", predictors))

# Fit logistic model
logmodel <- glm(log_formula, data = train, family = "binomial")

# Predict on test set
probs <- predict(logmodel, newdata = test, type = "response")
preds <- ifelse(probs > 0.5, 1, 0)

# Evaluate
test$Top_20 <- as.integer(test$Top_20 == TRUE)
test$preds <- preds


cm <- confusionMatrix(factor(test$preds, levels = c(0, 1)),
                      factor(test$Top_20, levels = c(0, 1)))

# Output results
summary(logmodel)
print(cm)

# Variable importance

imps_rand <- varImp(logmodel)
print(imps_rand)


```

*second model* the second model uses a timed split data (it uses all data before 2022 as train data and the rest as test data), and tries to use logistic regression to determine whether a country is in the top 20. this model performed better, at 92% accuracy. 

```{r second logistic regression model 12 parameters , echo=FALSE, warning=FALSE, message=FALSE}

# Split data by year
train_data <- subset(happiness_data, Year < 2022)
test_data <- subset(happiness_data, Year >= 2022)

# Predictor columns
cols <- c("Log_GDP", "Social_support", "Life_expectancy","Generosity","Inequality", "Pos_emotions", "Helped_stranger")

# Build formula
predictors <- paste(cols, collapse = " + ")
log_formula <- as.formula(paste("Top_20 ~", predictors))

# Fit logistic model
logmodel <- glm(log_formula, data = train, family = "binomial")

# Predict on test set
probs <- predict(logmodel, newdata = test, type = "response")
preds <- ifelse(probs > 0.5, 1, 0)

# Evaluate
test$Top_20 <- as.integer(test$Top_20 == TRUE)
test$preds <- preds

cm <- confusionMatrix(factor(test$preds, levels = c(0, 1)),
                      factor(test$Top_20, levels = c(0, 1)))

# Output results
summary(logmodel)
print(cm)

# Variable importance
imps_chron <- varImp(logmodel)
print(imps_chron)
# 1.Pos_emotions,2.Social_support,3.Inequality,4.Log_GDP,5.Helped_stranger,6.Life_expectancy,7.Generosity
```


```{r}
# Example data (replace with your actual vectors)
combined_df <- data.frame(val = imps_rand[[1]] + imps_chron[[1]])
combined_df_percent <- combined_df
combined_df_percent[[1]] <- round(combined_df[[1]] / sum(combined_df[[1]]) * 100, 2)
col_names_vec <- c("log_GDP", "Social_support", "Life_expectancy", "Generosity", 
               "Inequality", "Pos_emotions", "Helped_stranger")
combined_df_percent$name <- col_names_vec
print(new_avg_vec)

# shared factors in top 7 
#Log_GDP,social_support,pos_emotions,life exceptency
```

```{r,average ranking db}
country_summary <- happiness_data %>%
  group_by(Country_name) %>%
  summarise(
    Years_in_DB = n(),                         # Number of records (years) per country
    Avg_Rank = round(mean(Rank, na.rm = TRUE), 2)  # Average rank
  ) %>%
  arrange(Avg_Rank)  # Optional: sort by best rank

# Top 20 (best average rank)
top_25_countries_db <-head(country_summary, 25)
top_25_continent_vec <- c("Europe","Europe","Europe","Europe","Europe","Europe", "Asia", "Oceania","North America","Oceania","Europe","Central America","Europe","Europe","North America","Europe","Europe","Europe","North America","Europe","Europe","North America","Asia","Asia","South America")  

top_25_countries_db$Continent <- top_25_continent_vec
top_25_countries <- head(country_summary$Country_name, 25)


#Bottom 25 (worst average rank)
bottom_25_countries_db <- tail(country_summary, 25)
bottom_25_continent_vec <- c("Asia","Africa","Africa","Asia","Africa","Africa","Africa","Africa","Africa","Central America","Africa", "Africa","Africa","Africa","Africa","Africa","Africa","Africa","Asia","Africa","Africa","Africa","Asia","Africa","Africa")
bottom_25_countries_db$Continent <- bottom_25_continent_vec
bottom_25_countries <- tail(country_summary$Country_name, 25)



# Label each group
top_25_countries_db$Group <- "Top 25"
bottom_25_countries_db$Group <- "Bottom 25"

# Combine
continent_data <- bind_rows(top_25_countries_db, bottom_25_countries_db)

ggplot(continent_data, aes(x = Continent, fill = Continent)) +
  geom_bar() +
  facet_wrap(~ Group, nrow = 1) +  # facets in a row
  labs(title = "Continent Distribution of Top 25 and Bottom 25 Countries",
       x = "Continent", y = "Number of Countries") +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "none",
    panel.spacing = unit(2, "lines"),  # adds space between panels
    strip.background = element_rect(fill = "gray90", color = "black")
  )


```

```{r, progression of log_GDP}
#need to reclassify blue and red countries
set.seed(132)  # for reproducibility


random_15_top_logGDP <- sample(top_25_countries, 15)
random_15_bottom_logGDP <- sample(bottom_25_countries, 15)
blue_countries <- random_15_top_logGDP
red_countries  <- random_15_bottom_logGDP

selected_countries <- c(blue_countries, red_countries)


# Filter data
filtered_data <- happiness_data %>%
  filter(Country_name %in% selected_countries) %>%
  arrange(-Year)
# Add a color group variable
filtered_data <- filtered_data %>%
  mutate(Group = ifelse(Country_name %in% blue_countries, "Blue", "Red"))

lineplot<-ggplot(filtered_data, aes(x = Year, y = Log_GDP, color = Group, group = Country_name)) +
  geom_line(size = 1) +
  geom_point() +
  scale_color_manual(values = c("Blue" = "blue", "Red" = "red")) +
  labs(title = "log GDP Over Time (Selected Countries)",
       x = "Year", y = "log GDP Score") +
  theme_minimal()
lineplot
```

```{r, progression of social_support}
#need to reclassify blue and red countries
set.seed(124)  # for reproducibility


random_15_top_social_support <- sample(top_25_countries, 15)
random_15_bottom_social_support <- sample(bottom_25_countries, 15)
blue_countries <- random_15_top_social_support
red_countries  <- random_15_bottom_social_support

selected_countries <- c(blue_countries, red_countries)


# Filter data
filtered_data <- happiness_data %>%
  filter(Country_name %in% selected_countries) %>%
  arrange(-Year)
# Add a color group variable
filtered_data <- filtered_data %>%
  mutate(Group = ifelse(Country_name %in% blue_countries, "Blue", "Red"))

lineplot<-ggplot(filtered_data, aes(x = Year, y = Social_support , color = Group, group = Country_name)) +
  geom_line(size = 1) +
  geom_point() +
  scale_color_manual(values = c("Blue" = "blue", "Red" = "red")) +
  labs(title = "social support Over Time (Selected Countries)",
       x = "Year", y = "social support Score") +
  theme_minimal()
lineplot
```

```{r, progression of pos_emotions}
#need to reclassify blue and red countries
set.seed(125)  # for reproducibility


random_15_top_pos_emotions <- sample(top_25_countries, 15)
random_15_bottom_pos_emotions <- sample(bottom_25_countries, 15)
blue_countries <- random_15_top_pos_emotions
red_countries  <- random_15_bottom_pos_emotions

selected_countries <- c(blue_countries, red_countries)


# Filter data
filtered_data <- happiness_data %>%
  filter(Country_name %in% selected_countries) %>%
  arrange(-Year)
# Add a color group variable
filtered_data <- filtered_data %>%
  mutate(Group = ifelse(Country_name %in% blue_countries, "Blue", "Red"))

lineplot<-ggplot(filtered_data, aes(x = Year, y = Pos_emotions, color = Group, group = Country_name)) +
  geom_line(size = 1) +
  geom_point() +
  scale_color_manual(values = c("Blue" = "blue", "Red" = "red")) +
  labs(title = "pos_emotions Over Time (Selected Countries)",
       x = "Year", y = "pos_emotions Score") +
  theme_minimal()
lineplot
```

```{r, progression of life_expectancy}
#need to reclassify blue and red countries
set.seed(126)  # for reproducibility


random_15_top_life_expectancy <- sample(top_25_countries, 15)
random_15_bottom_life_expectancy <- sample(bottom_25_countries, 15)
blue_countries <- random_15_top_life_expectancy
red_countries  <- random_15_bottom_life_expectancy

selected_countries <- c(blue_countries, red_countries)


# Filter data
filtered_data <- happiness_data %>%
  filter(Country_name %in% selected_countries) %>%
  arrange(-Year)
# Add a color group variable
filtered_data <- filtered_data %>%
  mutate(Group = ifelse(Country_name %in% blue_countries, "Blue", "Red"))

lineplot<-ggplot(filtered_data, aes(x = Year, y = Life_expectancy, color = Group, group = Country_name)) +
  geom_line(size = 1) +
  geom_point() +
  scale_color_manual(values = c("Blue" = "blue", "Red" = "red")) +
  labs(title = "life_expectancy Over Time (Selected Countries)",
       x = "Year", y = "life_expectancy Score") +
  theme_minimal()
lineplot


```

```{r, progression of inequality}
#need to reclassify blue and red countries
set.seed(127)  # for reproducibility


random_15_top_pos_emotions <- sample(top_25_countries, 15)
random_15_bottom_pos_emotions <- sample(bottom_25_countries, 15)
blue_countries <- random_15_top_pos_emotions
red_countries  <- random_15_bottom_pos_emotions

selected_countries <- c(blue_countries, red_countries)

# Filter data
filtered_data <- happiness_data %>%
  filter(Country_name %in% selected_countries) %>%
  arrange(-Year)
# Add a color group variable
filtered_data <- filtered_data %>%
  mutate(Group = ifelse(Country_name %in% blue_countries, "Blue", "Red"))

lineplot<-ggplot(filtered_data, aes(x = Year, y = Inequality, color = Group, group = Country_name)) +
  geom_line(size = 1) +
  geom_point() +
  scale_color_manual(values = c("Blue" = "blue", "Red" = "red")) +
  labs(title = "Inequality Over Time (Selected Countries)",
       x = "Year", y = "life_expectancy Score") +
  theme_minimal()
lineplot
```

```{r, progression of corruption}
#need to reclassify blue and red countries
set.seed(128)  # for reproducibility


random_15_top_pos_emotions <- sample(top_25_countries, 15)
random_15_bottom_pos_emotions <- sample(bottom_25_countries, 15)
blue_countries <- random_15_top_pos_emotions
red_countries  <- random_15_bottom_pos_emotions

selected_countries <- c(blue_countries, red_countries)


# Filter data
filtered_data <- happiness_data %>%
  filter(Country_name %in% selected_countries) %>%
  arrange(-Year)
# Add a color group variable
filtered_data <- filtered_data %>%
  mutate(Group = ifelse(Country_name %in% blue_countries, "Blue", "Red"))

lineplot<-ggplot(filtered_data, aes(x = Year, y = Corruption, color = Group, group = Country_name)) +
  geom_line(size = 1) +
  geom_point() +
  scale_color_manual(values = c("Blue" = "blue", "Red" = "red")) +
  labs(title = "Corruption Over Time (Selected Countries)",
       x = "Year", y = "life_expectancy Score") +
  theme_minimal()
lineplot
```


\newpage

Appendix.\newline This Markdown file describes the data structure and organization for the project.\newline

we will also elaborate about the pre project data cleaning,\newline this data is a addition of WHR(world happiness reports) from 2011 to 2024.\newline the data cleaning included fixing the country names(where the standard of naming was changed in between the reports),erasing countries that dont exist and deleting duplicates.\newline we also changed the column names(not all of the column names were identical in all the reports.)\newline

Data Columns: \newline  
1. YEAR: the year in which the survey was conducted (the data is from 2011–2024)\newline  
2. Rank – the country's Rank in the according year (from 1 to 158); not all countries were ranked plus the rank is by 3 year average.\newline  
3. Country name – the abbreviated name of the relevant country.\newline  
4. Ladder score – the WHR provides a number of the final happiness score of the current country (from 1.3 to 7.9).\newline 
5. Rank – the Country's rank in this year.\newline  
6. Inequality – level of inequality in the country from(0.9 to 4.1).\newline 
7.Social support – a normalized version of the country's social support index in comparison to the rest of the world (normalized in range from 0 to 1.8).\newline  
8.GDP –  the country's GDP .\newline  
9. Healthy life expectancy – a normalized version of the country's healthy life expectancy index in comparison to the rest of the world (normalized in range from 0 to 1.13).\newline  
10. Freedom to make life choices – a normalized version of the country's freedom to make life choices index in comparison to the rest of the world (normalized in range from 0 to 1.018).\newline  
11. Generosity – a normalized version of the country's generosity index in comparison to the rest of the world (normalized in range from 0 to 0.57).\newline  
12. Perceptions of corruption – a normalized version of the country's perceptions of corruption index in comparison to the rest of the world (normalized in range from 0 to 0.587).\newline  
13.Pos_emotions- a normalized version of the Pos_emotions of the responders from the country(from 0.18 to 0.9)\newline
14.Neg_emotions- a normalized version of the Neg_emotions of the responders from the country(from 0.1 to 0.7)\newline
15.Donated - a normalized version of how many people donated from said country(from 0.003 to 0.92)\newline
16.Volunteered- a normalized version of how many people donated from said country(from 0.02 to 0.65)\newline
17.Helped_stranger- a normalized version of how many people helped a stranger from said country(from 0.115 to 0.88)\newline
18.Top 20 – a binary variable indicating whether the country is in the top 20 in that particular year (an outcome variable).\newline
19.Top_15- a binary variable indicating whether the country is in the top 15 in that particular year (an outcome variable).\newline
20.Top_25- a binary variable indicating whether the country is in the top 25 in that particular year (the outcome variable).\newline
21.Top20_Last- a numbered variable indicating whether the country was in the top 20 in that previous year (an outcome variable).\newline
22.Log_GDP -the country's GDP in log base 10\newline


Rows: Rows: 2,283\newline 
Columns: 21\newline 
$ Country_name    <chr> "Peru", "Tunisia", "Burkina Faso", "Australia", "Malta", "Israel", "Iraq", "Georgia", "Russian Federation", "DR Congo", "Philippines", "Indones…\newline
$ Year            <int> 2009, 2014, 2007, 2014, 2014, 2022, 2013, 2015, 2019, 2016, 2015, 2024, 2006, 2022, 2018, 2019, 2019, 2014, 2011, 2014, 2016, 2014, 2014, 2008,…\newline
$ Ladder_score    <dbl> 5.519, 4.764, 4.017, 7.289, 6.452, 7.662, 4.725, 4.122, 5.441, 4.522, 5.547, 5.573, 5.832, 5.870, 5.340, 5.674, 5.952, 6.037, 4.260, 4.240, 6.8…\newline
$ Rank            <int> 47, 98, 96, 9, 32, 2, 93, 122, 80, 105, 66, 84, 26, 67, 74, 69, 60, 44, 125, 119, 20, 114, 65, 74, 27, 42, 131, 97, 28, 143, 102, 67, 44, 126, …\newline
$ Inequality      <dbl> 2.2, 2.1, 1.3, 1.8, 1.9, 1.4, 3.0, 2.0, 2.3, 1.6, 2.7, 2.5, 2.4, 3.0, 2.5, 2.3, 2.0, 2.0, 1.9, 2.1, 2.0, 1.4, 2.4, 1.6, 1.5, 2.2, 1.7, 2.1, 2.1…\newline
$ Social_support  <dbl> 0.799, 0.680, 0.771, 0.924, 0.941, 0.954, 0.728, 0.517, 0.910, 0.864, 0.854, 0.814, 0.887, 0.868, 0.809, 0.784, 0.891, 0.932, 0.705, 0.778, 0.8…\newline
$ GDP             <dbl> 11206, 12753, 1732, 55253, 42958, 48196, 13713, 14937, 37699, 1253, 7692, 14474, 33613, 10072, 12262, 9985, 33473, 36218, 10990, 4588, 21572, 3…\newline
$ Life_expectancy <dbl> 68.10000, 66.10000, 49.50000, 70.10000, 70.70000, 70.62500, 61.00000, 64.10000, 63.70000, 52.30000, 61.30000, 61.91250, 62.70000, 63.93750, 63.…\newline
$ Freedom         <dbl> 0.6380000, 0.5890000, 0.5820000, 0.9230000, 0.9040000, 0.7750000, 0.5476667, 0.6400000, 0.7150000, 0.6370000, 0.9120000, 0.9070000, 0.8400000, …\newline
$ Generosity      <dbl> 0.202, 0.057, 0.095, 0.716, 0.781, 0.391, 0.241, 0.097, 0.241, 0.101, 0.212, 0.895, 0.499, 0.200, 0.807, 0.190, 0.076, 0.090, 0.058, 0.137, 0.1…\newline
$ Corruption      <dbl> 0.8800000, 0.7830000, 0.8330000, 0.4420000, 0.6700000, 0.6550000, 0.7100000, 0.5020000, 0.8480000, 0.8750000, 0.7550000, 0.8830000, 0.9170000, …\newline
$ Pos_emotions    <dbl> 0.7580000, 0.4240000, 0.6090000, 0.7400000, 0.6060000, 0.5830000, 0.5214667, 0.4480000, 0.6320000, 0.6100000, 0.7960000, 0.8420000, 0.7500000, …\newline
$ Neg_emotions    <dbl> 0.320, 0.321, 0.281, 0.245, 0.352, 0.183, 0.554, 0.233, 0.200, 0.222, 0.351, 0.285, 0.229, 0.269, 0.296, 0.419, 0.236, 0.151, 0.459, 0.216, 0.2…\newline
$ Donated         <dbl> 0.202, 0.057, 0.095, 0.716, 0.781, 0.391, 0.241, 0.097, 0.241, 0.101, 0.212, 0.895, 0.499, 0.200, 0.807, 0.190, 0.076, 0.090, 0.058, 0.137, 0.1…\newline
$ Volunteered     <dbl> 0.188, 0.057, 0.122, 0.396, 0.263, 0.196, 0.178, 0.184, 0.103, 0.104, 0.422, 0.654, 0.263, 0.397, 0.548, 0.201, 0.071, 0.194, 0.090, 0.125, 0.1…\newline
$ Helped_stranger <dbl> 0.423, 0.410, 0.375, 0.662, 0.501, 0.532, 0.745, 0.404, 0.449, 0.362, 0.552, 0.643, 0.626, 0.827, 0.552, 0.493, 0.417, 0.322, 0.427, 0.590, 0.4…\newline
$ Top_20          <lgl> FALSE, FALSE, FALSE, TRUE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE,…\newline
$ Top_15          <lgl> FALSE, FALSE, FALSE, TRUE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE…\newline
$ Top_25          <lgl> FALSE, FALSE, FALSE, TRUE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE,…\newline
$ Top20_Last      <dbl> 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 2, 0, 0, 1, 0, 1,…\newline
$ Log_GDP         <dbl> 9.324205, 9.453522, 7.457032, 10.919678, 10.667978, 10.783031, 9.526100, 9.611597, 10.537389, 7.133296, 8.947936, 9.580109, 10.422668, 9.217515…\newline