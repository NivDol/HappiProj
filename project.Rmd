---
title: 'Project Proposal: Happiness Report’s top tier for multiple consecutive years'
author: "Amal Dagan, Bar Evron, Niv Dolev, Roni London"
output:
  html_document:
    df_print: paged
  pdf_document:
    number_sections: true
    fig_caption: true
    latex_engine: xelatex
---

\newpage

```{r, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```

```{r libreries}
library(tidyverse)
library(dplyr)
library(stringr)
library(ggplot2)
library(patchwork)
library(caret)
library(pROC)
library(rpart.plot)
library(readxl)  # a libary for loading excel files
```

# Introduction

**Research Question**\
Which indicators enable a country to remain in the World Happiness Report’s top tier for multiple consecutive years?

**Background**\
GDP per capita, social support, life expectancy and perceived freedom are repeatedly cited as key drivers of well-being.

**Current Gap and existing research**\
Several studies have examined national happiness using WHR data, though few have explored long-term patterns. For example, Kuppens et al. (2023) analyzed cultural factors in 78 countries before and after the COVID-19 pandemic. They found that individualism and indulgence became stronger predictors of happiness post-COVID, suggesting that global crises may shift the importance of cultural values. However, their analysis compared only two time points (2017–2019 vs. 2021) and did not assess whether these associations hold across a longer period. Similarly, Al‐Maatouq and Al Shammari (2022) examined the relationship between WHR scores, Hofstede’s cultural dimensions, and government education spending across 58 countries. They found that countries with higher long-term orientation and lower power distance tended to score higher on the WHR, indicating possible structural links between culture and well-being. However, their study was cross-sectional and did not account for how such associations might change over time. Despite the value of these contributions, our research takes a unique direction. We combine an unusually broad dataset—spanning 2011 to 2024 and covering over 150 countries—with repeated-measures modeling to assess which happiness predictors remain robust over time. Unlike studies that focus on a single year or a narrow method, our approach allows us to explore both consistency and change in global well-being, providing updated and policy-relevant insights.

**Contribution**\
Turning the dataset’s continual renewal from hurdle to asset, we deliver up-to-date policy guidance and reveal whether the perennial “Top-20” countries stay happy for the same reasons or for shifting, time-dependent ones.

# Data

We use publicly available data from the World Happiness Report, based on the Gallup World Poll, covering the years 2011 to 2023. The dataset contains 1,969 yearly observations across 125 to 153 countries per year, with 14 variables per country-year entry. Each observation represents a single country in a given year. The variables include social, health, and economic factors widely studied in happiness research, such as: social support, GDP per capita, healthy life expectancy, freedom to make life choices, generosity, and perceptions of corruption. Our binary outcome variable indicates whether a country was ranked in the Top 20 happiest countries that year. Importantly, the happiness score itself is not a single-year estimate but a three-year rolling average, meaning each year’s score reflects data from the current and two previous years. For example, the 2022 score is an average of survey results from 2020, 2021, and 2022. This design introduces temporal dependence between consecutive years. To address this, we plan to either: use repeated-measures models (e.g., mixed-effects models) that account for within-country correlations across time, or aggregate data at the country level to study long-term patterns, such as overall average happiness or total number of Top 20 appearances.

A detailed data dictionary is included in the /data/README.md file in our repository.

# Preliminary Results

```{r loading data}
happiness_data <- read.csv("C:/Users/Niv Dolev/Documents/merged_whr_data.csv")
happiness_data <- happiness_data %>% 
  filter(!is.na(Average..annual.)) 

hist(happiness_data$GDP)

null_countries <- c("China", "Kosovo", "South Sudan", "State of Palestine", "Turkmenistan", "United Arab Emirates", "Saudi Arabia", "Kuwait", "Bahrain", "Jordan", "Côte d’Ivoire", "Hong Kong SAR of China", "Netherlands", "Taiwan Province of China", "Cuba", "Maldives", "Oman", "Sudan")

better_data = subset(happiness_data, !(Country %in% null_countries))

happiness_data <- better_data %>%
  group_by(Country) %>%
  mutate(across(
    where(is.numeric),
    ~ ifelse(is.na(.), mean(., na.rm = TRUE), .)
  )) %>%
  ungroup()
```

```{r plots-and-table, fig.width=9, fig.height=5, echo=FALSE, warning=FALSE, message=FALSE}

happiness_data <- happiness_data %>%
  group_by(Year) %>%
  mutate(Top_20 = Average..annual..rank <= 20) %>%# create a new top_20 column
  mutate(Top_15 = Average..annual..rank <= 15) %>%# create a new top_15 column
  mutate(Top_25 = Average..annual..rank <= 25) %>%# create a new top_25 column
  
  ungroup() %>%
  #renaming the column in order to be usable in R
  rename(Country_name = Country, Ladder_score = Average..annual., GDP = GDP.per.capita, Social_support = Social.support, Life_expectancy = Healthy.life.expectancy, Corruption = Perceptions.of.corruption, Rank = Average..annual..rank, Pos_emotions = Positive.emotions, Neg_emotions = Negative.emotions, Helped_stranger = Helped.a.stranger)
happiness_data <- happiness_data %>%
  arrange(Country_name, Year) %>%  # Sort by country and year
  group_by(Country_name) %>%      # Process each country separately
  mutate(Top20_Last = lag(Top_20)) %>%  # Get last year's Top_20 and create new column
  ungroup() %>%  # Remove grouping
  mutate(Top20_Last = case_when(
    is.na(Top20_Last) ~ 2,         # No previous data
    Top20_Last == 1 ~ 1,           # Was in top 20
    Top20_Last == 0 ~ 0            # Was not in top 20
  ))
happiness_data$Log_GDP <- log(happiness_data$GDP)

summary(happiness_data)

# count Top_20 appearences by country
top_countries_long <- happiness_data %>%
  filter(Top_20) %>%
  count(Country_name, name = "Years_in_Top_20") %>%
  arrange(desc(Years_in_Top_20))

tpl1 <- top_countries_long %>%
  filter(Years_in_Top_20 >= 5) %>% 
  ggplot(aes(x = reorder(Country_name, Years_in_Top_20), y = Years_in_Top_20)) +
  geom_col(fill = "lightgreen") +
  coord_flip() +
  labs(
    title = "Number of Years Each Country was in the Top 20",
    x = "Country",
    y = "Years in Top 20"
  ) +
  theme_minimal(base_size = 13) +
  theme(
    plot.title = element_text(size = 16, face = "bold"),
    axis.text = element_text(size = 12),
    axis.title = element_text(size = 13)
  )

plt1 <- ggplot(happiness_data, aes(x = Social_support, y = Ladder_score)) +
  geom_point(alpha=0.4) +
  geom_smooth(method="lm", se=FALSE, color="blue") +
  labs(title="Ladder score vs Social support")

plt <- ggplot(happiness_data, aes(x=Rank)) + geom_histogram(color="darkblue", fill="lightblue", binwidth=3) +
  labs(title="Distribution of rankings")+
  theme_minimal(base_size = 13) +
  theme(
    plot.title = element_text(size = 12, face = "bold"),
    axis.text = element_text(size = 12),
    axis.title = element_text(size = 13)
  )
```

```{r top_countries_long, fig.width=10, fig.height=5, echo=FALSE, warning=FALSE, message=FALSE}
tpl1 | plt
```

The distribution of the number of years in the top 20 is kept pretty limited, only 14 countries have never left the top 20 spots. This goes to show that being a top country is a perpetual title. Which means there is merit to asking this question, we think there is a pattern of data that indicates how consistent a country is at staying at the top of the list.

# Work Plan

The outcome is the Top-20 column and the predictors currently are all of the columns. The way we will answer the question is that we will attempt to find the most contributing variables in order to answer the question.

we will use a variety of methods to answer the question: SHAP variables,decision-tree and a logistic regression model, we will attempt to build those models concurrently and to compare and contrast them.

we would like to receive some conclusive results(the p_value of some of the variables will be significant),in all models so after comparison we could isolate the most relevant factors.

-   Data preparation and cleaning. (Bar)
-   Exploratory analysis and visualization. (Roni)
-   Model building and evaluation. (Roni,Niv)
-   Interpretation of results and final reporting. (Amal,Niv)

\\\

**models and fun** The main model: Decision Tree

Like logistic regression this is also a popular model for classification problems, and since it can be visualized it is an obvious choice to be one of the models.

The second model example: Logistic Regression

If we're to take a classification route, then logistic regression would be a good model for that. We got good results, but again maybe too good. All of the predictors seem to be strong predictors which was good for the sake of showing that there is a correlation, but the performance was too accurate to use as the main model.

*decision tree models*

we will create 2 sets of 3 diffrent tree models
the first set will include the following models: 
the first model will be a decision tree predicting the top_15 parameter and will use the chronologically split data.
the second model will be a decision tree predicting the top_20 parameter and will use the chronologically split data. 
the third model will be a decision tree predicting the top_25 parameter and will use the chronologically split data. 

the second set will include the following models: 
the first model will be a decision tree predicting the top_15 parameter and will use the randomly split data.
the second model will be a decision tree predicting the top_20 parameter and will use the randomly split data. 
the third model will be a decision tree predicting the top_25 parameter and will use the randomly split data. 

we start with chronologically spliting the data for prior and post 2022 in an almost 80-20 split

```{r first_data_split}

# Split data
train_data_chron <- subset(happiness_data, Year < 2022)
test_data_chron <- subset(happiness_data, Year >= 2022)

```
**using 11 parameters**

*first set*

*first model*

the first model training and pie chart of the factor importance

```{r first_model_training 12 parameters}
first_tree_model_12 <- rpart(
  Top_15 ~ GDP + Inequality + Social_support + Life_expectancy + Pos_emotions +
    Freedom + Generosity + Corruption + Donated + Volunteered +Helped_stranger+Neg_emotions,
  data = train_data_chron,
  method = "class"
)
rpart.plot(first_tree_model_12)# the tree

importance_1_12 <- first_tree_model_12$variable.importance# variable importance scores

pie(importance_1_12,
    main = "Variable Importance (Decision Tree)",
    col = rainbow(length(importance_1_12)))
# variable importance pie chart for better visual dislay of importance
```

the first tree model predictions had a nearly 84% accuracy which shows that the model is good and using the parameters can reliably predict if a country is in the top_15 

```{r first_model_predict 12 parameters}

test_data_chron$Top_15 <- as.factor(test_data_chron$Top_15)
# Predict class labels (Top_20) for test_data

predictions <- predict(first_tree_model_12, newdata = test_data_chron, type = "class")
# Confusion matrix

conf_matrix <- table(Predicted = predictions, Actual = test_data_chron$Top_15)
print(conf_matrix)

# Accuracy
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
cat("Test Accuracy:", round(accuracy, 4), "\n")

```

*second model*
the second model training and pie chart of the factor importance

```{r second_model_training 12 parameters}

# Train the decision tree using rpart
second_tree_model_12 <- rpart(Top_20 ~ GDP + Inequality + Social_support + Life_expectancy + Pos_emotions + Freedom + Generosity + Corruption + Donated + Volunteered +Helped_stranger +Neg_emotions ,data = train_data_chron, method = "class")
rpart.plot(second_tree_model_12)

importance_2_12 <- second_tree_model_12$variable.importance

pie(importance_2_12,
    main = "Variable Importance (Decision Tree)",
    col = rainbow(length(importance_2_12)))
# variable importance pie chart for better visual dislay of importance
```

the second model predictions which shows an 82.5% which is a good model, and shows that  we are able to predict with reliable accuracy the Top_20 factor.

```{r second_model_predict 12 parameters}

test_data_chron$Top_20 <- as.factor(test_data_chron$Top_20)
# Predict class labels (Top_20) for test_data
predictions <- predict(second_tree_model_12, newdata = test_data_chron, type = "class")
# Confusion matrix
conf_matrix <- table(Predicted = predictions, Actual = test_data_chron$Top_20)
print(conf_matrix)

# Accuracy
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
cat("Test Accuracy:", round(accuracy, 4), "\n")

```

*third model*

the third model training and pie chart of the factor importance best model

```{r third_model_training 12 parameters}

third_tree_model_12 <- rpart(Top_25 ~  GDP + Inequality + Social_support + Life_expectancy + Pos_emotions + Freedom + Generosity + Corruption + Donated + Volunteered +Helped_stranger+Neg_emotions ,data = train_data_chron, method = "class")
rpart.plot(third_tree_model_12)

importance_3_12 <- third_tree_model_12$variable.importance

pie(importance_3_12,
    main = "Variable Importance (Decision Tree)",
    col = rainbow(length(importance_3_12)))
# variable importance pie chart for better visual dislay of importance

```

the third tree model predictions had a nearly 95% accuracy which shows that the model is too good and using the parameters we can not reliably predict if a country is in the top_25.

```{r third_model_predict 12 parameters}

test_data_chron$Top_25 <- as.factor(test_data_chron$Top_25)
# Predict class labels (Top_20) for test_data
predictions <- predict(third_tree_model_12, newdata = test_data_chron, type = "class")
# Confusion matrix
conf_matrix <- table(Predicted = predictions, Actual = test_data_chron$Top_25)
print(conf_matrix)

# Accuracy
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
cat("Test Accuracy:", round(accuracy, 4), "\n")

```
Top 7 factors are: GDP,Corruption,Social_support,Freedom,Pos_emotions,Generosity    Donated.
```{r}
# Example data (replace with your actual vectors)
timevec <- (importance_1_12+importance_2_12+importance_3_12)
timevec <-timevec/sum(timevec) * 100
print(timevec)
```

we will then split the data randomly and check if the results differ.

```{r second-data_split}

happiness_data <- happiness_data[sample(nrow(happiness_data)), ]#take the happiness data
sample <- sample(c(TRUE, FALSE), nrow(happiness_data), replace=TRUE, prob=c(0.8,0.2))# split the data 80-20
train_data_rand  <- happiness_data[sample, ]
test_data_rand   <- happiness_data[!sample, ]

```
*second set*

*fourth model*

the fourth model training and pie chart of the factor importance

```{r fourth_model_training 12 parameters}

fourth_tree_model_12 <- rpart(
  Top_15 ~ GDP + Inequality + Social_support + Life_expectancy + Pos_emotions + Freedom + Generosity + Corruption + Donated + Volunteered +Helped_stranger+Neg_emotions,data = train_data_rand,
  method = "class"
)
rpart.plot(fourth_tree_model_12)# the tree

importance_4 <- fourth_tree_model_12$variable.importance# variable importance scores
print(importance_4)

pie(importance_4,
    main = "Variable Importance (Decision Tree)",
    col = rainbow(length(importance_4)))
# variable importance pie chart for better visual dislay of importance
```

the fourth tree model predictions had a nearly 98% accuracy which is way higher than 84% of the correspending model and shows that the model is almost perfect and that the split by time is crucial for creating a good model.

```{r fourth_model_predict 12 parameters}

test_data_rand$Top_15 <- as.factor(test_data_rand$Top_15)
# Predict class labels (Top_20) for test_data

predictions <- predict(fourth_tree_model_12, newdata = test_data_rand, type = "class")
# Confusion matrix

conf_matrix <- table(Predicted = predictions, Actual = test_data_rand$Top_15)
print(conf_matrix)

# Accuracy
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
cat("Test Accuracy:", round(accuracy, 4), "\n")

```

*fifth model*
the fifth model training and pie chart of the factor importance

```{r fifth_model_training 12 parameters}

# Train the decision tree using rpart
fifth_tree_model_12 <- rpart(Top_20 ~  GDP + Inequality + Social_support + Life_expectancy + Pos_emotions + Freedom + Generosity + Corruption + Donated + Volunteered +Helped_stranger+Neg_emotions ,data = train_data_rand, method = "class")
rpart.plot(fifth_tree_model_12)

importance_5 <- fifth_tree_model_12$variable.importance
print(importance_5)

pie(importance_5,
    main = "Variable Importance (Decision Tree)",
    col = rainbow(length(importance_5)))
# variable importance pie chart for better visual dislay of importance
```

the fifth model predictions which shows at about 96% which shows like the previous model that the random models have unreliable accraucy.

```{r fifth_model_predict 12 parameters}

test_data_rand$Top_20 <- as.factor(test_data_rand$Top_20)
# Predict class labels (Top_20) for test_data
predictions <- predict(fifth_tree_model_12, newdata = test_data_rand, type = "class")
# Confusion matrix
conf_matrix <- table(Predicted = predictions, Actual = test_data_rand$Top_20)
print(conf_matrix)

# Accuracy
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
cat("Test Accuracy:", round(accuracy, 4), "\n")

```

*sixth model*

the sixth model training and pie chart of the factor importance

```{r sixth_model_training 12 parameters}

sixth_tree_model_12 <- rpart(Top_25 ~  GDP + Inequality + Social_support + Life_expectancy + Pos_emotions + Freedom + Generosity + Corruption + Donated + Volunteered + Helped_stranger+Neg_emotions ,data = train_data_rand, method = "class")
rpart.plot(sixth_tree_model_12)

importance_6 <- sixth_tree_model_12$variable.importance
print(importance_6)

pie(importance_6,
    main = "Variable Importance (Decision Tree)",
    col = rainbow(length(importance_6)))
# variable importance pie chart for better visual dislay of importance

```

the third model predictions which shows at about 94% accraucy and knowing how the model works shows that at all 3 ranges the randomized models are too sucsseful. 

```{r sixth_model_predict 12 parameters}

test_data_rand$Top_25 <- as.factor(test_data_rand$Top_25)
# Predict class labels (Top_20) for test_data
predictions <- predict(sixth_tree_model_12, newdata = test_data_rand, type = "class")
# Confusion matrix
conf_matrix <- table(Predicted = predictions, Actual = test_data_rand$Top_25)
print(conf_matrix)

# Accuracy
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
cat("Test Accuracy:", round(accuracy, 4), "\n")

```
Top 7 factors are: GDP,Corruption,Social_support,Freedom,Pos_emotions,Generosity    Donated.
```{r average 11 parameter random model importance}
# Example data (replace with your actual vectors)
randvec <- (importance_4+importance_5+importance_6)
randvec <-randvec/sum(randvec) * 100
print(randvec)
```
Top 7 factors in tree models:
GDP,Corruption,Social_support,Pos_emotions,Donated,Life_expectancy,Freedom
```{r average 11 parameter random and chron model importance}
# Example data (replace with your actual vectors)
avg_vec<-(randvec+timevec)/2
print(sort(avg_vec,decreasing = TRUE))
```
**7 parameters**

new tree models based on 7 parameters

*first set*

*first model*

```{r first_model_training 7 parameters}
first_tree_model_7 <- rpart(
  Top_15 ~ GDP + Social_support + Life_expectancy + Pos_emotions +
    Freedom + Corruption + Volunteered,
  data = train_data_chron,
  method = "class"
)
rpart.plot(first_tree_model_7)# the tree

importance_1_7 <- first_tree_model_7$variable.importance# variable importance scores

pie(importance_1_7,
    main = "Variable Importance (Decision Tree)",
    col = rainbow(length(importance_1_7)))
# variable importance pie chart for better visual dislay of importance
```

the first tree model predictions had a nearly 84% accuracy which shows that the model is good and using the parameters can reliably predict if a country is in the top_15 

```{r first_model_predict 7 parameters}

test_data_chron$Top_15 <- as.factor(test_data_chron$Top_15)
# Predict class labels (Top_20) for test_data

predictions <- predict(first_tree_model_7, newdata = test_data_chron, type = "class")
# Confusion matrix

conf_matrix <- table(Predicted = predictions, Actual = test_data_chron$Top_15)
print(conf_matrix)

# Accuracy
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
cat("Test Accuracy:", round(accuracy, 4), "\n")

```

*second model*

the second model training and pie chart of the factor importance

```{r second_model_training 7 parameters}

# Train the decision tree using rpart
second_tree_model_7 <- rpart(Top_20 ~ GDP + Social_support + Life_expectancy + Pos_emotions +Freedom + Corruption + Volunteered ,data = train_data_chron, method = "class")
rpart.plot(second_tree_model_7)

importance_2_7 <- second_tree_model_7$variable.importance

pie(importance_2_7,
    main = "Variable Importance (Decision Tree)",
    col = rainbow(length(importance_2_7)))
# variable importance pie chart for better visual dislay of importance
```

the second model predictions which shows an 82.5% which is a good model, and shows that  we are able to predict with reliable accuracy the Top_20 factor.

```{r second_model_predict 7 parameters}

test_data_chron$Top_20 <- as.factor(test_data_chron$Top_20)
# Predict class labels (Top_20) for test_data
predictions <- predict(second_tree_model_7, newdata = test_data_chron, type = "class")
# Confusion matrix
conf_matrix <- table(Predicted = predictions, Actual = test_data_chron$Top_20)
print(conf_matrix)

# Accuracy
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
cat("Test Accuracy:", round(accuracy, 4), "\n")

```

*third model*

the third model training and pie chart of the factor importance best model

```{r third_model_training 7 parameters}

third_tree_model_7 <- rpart(Top_25 ~  GDP + Social_support + Life_expectancy + Pos_emotions +Freedom + Corruption + Volunteered ,data = train_data_chron, method = "class")
rpart.plot(third_tree_model_7)

importance_3_7 <- third_tree_model_7$variable.importance

pie(importance_3_7,
    main = "Variable Importance (Decision Tree)",
    col = rainbow(length(importance_3_11)))
# variable importance pie chart for better visual dislay of importance

```

the third tree model predictions had a nearly 95% accuracy which shows that the model is too good and using the parameters we can not reliably predict if a country is in the top_25.

```{r third_model_predict 7 parameters}

test_data_chron$Top_25 <- as.factor(test_data_chron$Top_25)
# Predict class labels (Top_20) for test_data
predictions <- predict(third_tree_model_7, newdata = test_data_chron, type = "class")
# Confusion matrix
conf_matrix <- table(Predicted = predictions, Actual = test_data_chron$Top_25)
print(conf_matrix)

# Accuracy
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
cat("Test Accuracy:", round(accuracy, 4), "\n")

```
Top 7 factors are: GDP,Corruption,Social_support,Freedom,Pos_emotions,Donated,Life_expectancy.  
.
```{r}
# Example data (replace with your actual vectors)
newtimevec <- (importance_1_7+importance_2_7+importance_3_7)
newtimevec <-newtimevec/sum(newtimevec) * 100
print(newtimevec)
```
*second set*

*fourth model*

the fourth model training and pie chart of the factor importance

```{r fourth_model_training 7 parameters}

fourth_tree_model_7 <- rpart(
  Top_15 ~ GDP + Social_support + Life_expectancy + Pos_emotions +Freedom + Corruption + Volunteered,data = train_data_rand,method = "class"
)
rpart.plot(fourth_tree_model_7)# the tree

importance_4_7 <- fourth_tree_model_7$variable.importance# variable importance scores

pie(importance_4_7,
    main = "Variable Importance (Decision Tree)",
    col = rainbow(length(importance_4_7)))
# variable importance pie chart for better visual dislay of importance
```

the fourth tree model predictions had a nearly 98% accuracy which is way higher than 84% of the correspending model and shows that the model is almost perfect and that the split by time is crucial for creating a good model.

```{r fourth_model_predict 7 parameters}

test_data_rand$Top_15 <- as.factor(test_data_rand$Top_15)
# Predict class labels (Top_20) for test_data

predictions <- predict(fourth_tree_model_7, newdata = test_data_rand, type = "class")
# Confusion matrix

conf_matrix <- table(Predicted = predictions, Actual = test_data_rand$Top_15)
print(conf_matrix)

# Accuracy
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
cat("Test Accuracy:", round(accuracy, 4), "\n")

```

*fifth model*
the fifth model training and pie chart of the factor importance

```{r fifth_model_training 7 parameters}

# Train the decision tree using rpart
fifth_tree_model_7 <- rpart(Top_20 ~  GDP + Social_support + Life_expectancy + Pos_emotions +Freedom + Corruption + Volunteered ,data = train_data_rand, method = "class")
rpart.plot(fifth_tree_model_7)

importance_5_7 <- fifth_tree_model_7$variable.importance


pie(importance_5_7,
    main = "Variable Importance (Decision Tree)",
    col = rainbow(length(importance_5_7)))
# variable importance pie chart for better visual dislay of importance
```

the fifth model predictions which shows at about 96% which shows like the previous model that the random models have unreliable accraucy.

```{r fifth_model_predict 7 parameters}

test_data_rand$Top_20 <- as.factor(test_data_rand$Top_20)
# Predict class labels (Top_20) for test_data
predictions <- predict(fifth_tree_model_7, newdata = test_data_rand, type = "class")
# Confusion matrix
conf_matrix <- table(Predicted = predictions, Actual = test_data_rand$Top_20)
print(conf_matrix)

# Accuracy
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
cat("Test Accuracy:", round(accuracy, 4), "\n")

```

*sixth model*

the sixth model training and pie chart of the factor importance

```{r sixth_model_training 7 parameters}

sixth_tree_model_7 <- rpart(Top_25 ~  GDP + Social_support + Life_expectancy + Pos_emotions +Freedom + Corruption + Volunteered ,data = train_data_rand, method = "class")
rpart.plot(sixth_tree_model_7)

importance_6_7 <- sixth_tree_model_7$variable.importance


pie(importance_6_7,
    main = "Variable Importance (Decision Tree)",
    col = rainbow(length(importance_6_7)))
# variable importance pie chart for better visual dislay of importance

```

the third model predictions which shows at about 94% accraucy and knowing how the model works shows that at all 3 ranges the randomized models are too sucsseful. 

```{r sixth_model_predict 7 parameters}

test_data_rand$Top_25 <- as.factor(test_data_rand$Top_25)
# Predict class labels (Top_20) for test_data
predictions <- predict(sixth_tree_model_7, newdata = test_data_rand, type = "class")
# Confusion matrix
conf_matrix <- table(Predicted = predictions, Actual = test_data_rand$Top_25)
print(conf_matrix)

# Accuracy
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
cat("Test Accuracy:", round(accuracy, 4), "\n")

```
```{r}
# Example data (replace with your actual vectors)
newrandvec <- (importance_4_7+importance_5_7+importance_6_7)
newrandvec <-newrandvec/sum(newrandvec) * 100
print(newrandvec)
```
Top 7 factors in tree models:
GDP,Corruption,Social_support,Pos_emotions,Donated,Life_expectancy,Freedom
```{r}
# Example data (replace with your actual vectors)
new_avg_vec<-(newrandvec+newtimevec)/2
new_avg_vec<-sort(new_avg_vec,decreasing = TRUE)
print(new_avg_vec)
```

# Combine into one data frame for ggplot
library(tibble)
library(dplyr)

df <- bind_rows(
  tibble(Stat = names(vec1), Value = vec1, Group = "Set 1"),
  tibble(Stat = names(vec2), Value = vec2, Group = "Set 2"),
  tibble(Stat = names(vec3), Value = vec3, Group = "Set 3"),
  tibble(Stat = names(vec4), Value = vec4, Group = "Set 4")
)


*logistical regression models*

in this section we created two logistical regression models which differ in the test and train data they recieve.

*first logistical regression model* the first model uses a randomly split data and tries to use logistical regression to determine whether a country is in the top 20. this model is to successful at 97.5% accuracy deeming it ineffective and unreliable.

```{r  first logistic regression model 12 parameters , echo=FALSE, warning=FALSE, message=FALSE}
set.seed(123)

# Split data into train/test
happiness_data <- happiness_data[sample(nrow(happiness_data)), ]
sample <- sample(c(TRUE, FALSE), nrow(happiness_data), replace = TRUE, prob = c(0.8, 0.2))
train <- happiness_data[sample, ]
test <- happiness_data[!sample, ]

# Predictor columns
cols <- c("Log_GDP", "Social_support", "Life_expectancy", "Freedom", 
          "Generosity", "Corruption", "Inequality", "Pos_emotions", 
          "Neg_emotions", "Donated", "Volunteered", "Helped_stranger")

# Build formula
predictors <- paste(cols, collapse = " + ")
log_formula <- as.formula(paste("Top_20 ~", predictors))

# Fit logistic model
logmodel <- glm(log_formula, data = train, family = "binomial")

# Predict on test set
probs <- predict(logmodel, newdata = test, type = "response")
preds <- ifelse(probs > 0.5, 1, 0)

# Evaluate
test$Top_20 <- as.integer(test$Top_20 == TRUE)
test$preds <- preds

library(caret)
cm <- confusionMatrix(factor(test$preds, levels = c(0, 1)),
                      factor(test$Top_20, levels = c(0, 1)))

# Output results
summary(logmodel)
print(cm)

# Variable importance
library(caret)
imps <- varImp(logmodel)
print(imps)
#1.Pos_emotions,2.Social_support,3.Inequality,4.Log_GDP,5.Life_expectancy,6.Helped_stranger,7.Generosity

```
7 factor random logistical regression model
```{r  first logistic regression model 7 parameters , echo=FALSE, warning=FALSE, message=FALSE}
set.seed(123)

# Split data into train/test
happiness_data <- happiness_data[sample(nrow(happiness_data)), ]
sample <- sample(c(TRUE, FALSE), nrow(happiness_data), replace = TRUE, prob = c(0.8, 0.2))
train <- happiness_data[sample, ]
test <- happiness_data[!sample, ]


# Predictor columns
cols <- c("Log_GDP", "Social_support", "Life_expectancy","Generosity","Inequality", "Pos_emotions", "Helped_stranger")

# Build formula
predictors <- paste(cols, collapse = " + ")
log_formula <- as.formula(paste("Top_20 ~", predictors))

# Fit logistic model
logmodel <- glm(log_formula, data = train, family = "binomial")

# Predict on test set
probs <- predict(logmodel, newdata = test, type = "response")
preds <- ifelse(probs > 0.5, 1, 0)

# Evaluate
test$Top_20 <- as.integer(test$Top_20 == TRUE)
test$preds <- preds


cm <- confusionMatrix(factor(test$preds, levels = c(0, 1)),
                      factor(test$Top_20, levels = c(0, 1)))

# Output results
summary(logmodel)
print(cm)

# Variable importance

imps_rand <- varImp(logmodel)
print(imps_rand)


```

*second model* the second model uses a timed split data(it uses all data before 2022 as train data and the rest as test data), and tries to use logistical regression to determine whether a country is in the top 20. this model as well is to successful at 96% accuracy deeming it ineffective and unreliable as well.

```{r second logistic regression model 12 parameters , echo=FALSE, warning=FALSE, message=FALSE}

# Split data by year
train_data <- subset(happiness_data, Year < 2022)
test_data <- subset(happiness_data, Year >= 2022)

# Predictor columns
cols <- c("Log_GDP", "Social_support", "Life_expectancy","Generosity","Inequality", "Pos_emotions", "Helped_stranger")

# Build formula
predictors <- paste(cols, collapse = " + ")
log_formula <- as.formula(paste("Top_20 ~", predictors))

# Fit logistic model
logmodel <- glm(log_formula, data = train, family = "binomial")

# Predict on test set
probs <- predict(logmodel, newdata = test, type = "response")
preds <- ifelse(probs > 0.5, 1, 0)

# Evaluate
test$Top_20 <- as.integer(test$Top_20 == TRUE)
test$preds <- preds

cm <- confusionMatrix(factor(test$preds, levels = c(0, 1)),
                      factor(test$Top_20, levels = c(0, 1)))

# Output results
summary(logmodel)
print(cm)

# Variable importance
imps_chron <- varImp(logmodel)
print(imps_chron)
# 1.Pos_emotions,2.Social_support,3.Inequality,4.Log_GDP,5.Helped_stranger,6.Life_expectancy,7.Generosity
```


```{r}
# Example data (replace with your actual vectors)
combined_df <- data.frame(val = imps_rand[[1]] + imps_chron[[1]])
combined_df_percent <- combined_df
combined_df_percent[[1]] <- round(combined_df[[1]] / sum(combined_df[[1]]) * 100, 2)
col_names_vec <- c("log_GDP", "Social_support", "Life_expectancy", "Generosity", 
               "Inequality", "Pos_emotions", "Helped_stranger")
combined_df_percent$name <- col_names_vec
print(combined_df_percent)
print(new_avg_vec)

# shared factors in top 7 
#Log_GDP,social_support,pos_emotions,life exceptency
```


```{r, progression of corruption}
#need to reclassify blue and red countries
#niv idea- green countries are 15 countries with best average rank and red countries with worst average rating.

blue_countries <- c("Sweden", "Denmark", "New Zealand", "Israel", "Ireland")
red_countries  <- c("Madagascar", "Zambia", "Ethiopia", "Sri Lanka", "Bangladesh")

selected_countries <- c(blue_countries, red_countries)

# Filter data
filtered_data <- happiness_data %>%
  filter(Country_name %in% selected_countries) %>%
  arrange(-Year)
# Add a color group variable
filtered_data <- filtered_data %>%
  mutate(Group = ifelse(Country_name %in% blue_countries, "Blue", "Red"))

lineplot<-ggplot(filtered_data, aes(x = Year, y = Corruption, color = Group, group = Country_name)) +
  geom_line(size = 1) +
  geom_point() +
  scale_color_manual(values = c("Blue" = "blue", "Red" = "red")) +
  labs(title = "Corruption Over Time (Selected Countries)",
       x = "Year", y = "Corruption Score") +
  theme_minimal()
lineplot
```

```{r}
blue_countries <- c("Sweden", "Denmark", "New Zealand", "Israel", "Ireland")
red_countries  <- c("Madagascar", "Zambia", "Ethiopia", "Sri Lanka", "Bangladesh")

selected_countries <- c(blue_countries, red_countries)

# Filter data
filtered_data <- happiness_data %>%
  filter(Country_name %in% selected_countries) %>%
  arrange(-Year)
# Add a color group variable
filtered_data <- filtered_data %>%
  mutate(Group = ifelse(Country_name %in% blue_countries, "Blue", "Red"))

lineplot<-ggplot(filtered_data, aes(x = Year, y = Log_GDP, color = Group, group = Country_name)) +
  geom_line(size = 1) +
  geom_point() +
  scale_color_manual(values = c("Blue" = "blue", "Red" = "red")) +
  labs(title = "GDP Over Time (Selected Countries)",
       x = "Year", y = "Donated") +
  theme_minimal()
lineplot
```






\newpage

Appendix.\newline This Markdown file describes the data structure and organization for the project.\newline

we will also elaborate about the pre project data cleaning,\newline this data is a addition of WHR(world happiness reports) from 2011 to 2024.\newline the data cleaning included fixing the country names(where the standard of naming was changed in between the reports),erasing countries that dont exist and deleting duplicates.\newline we also changed the column names(not all of the column names were identical in all the reports.)\newline

Data Columns: \newline  
1. YEAR: the year in which the survey was conducted (the data is from 2011–2024)\newline  
2. Rank – the country's Rank in the according year (from 1 to 158); not all countries were ranked plus the rank is by 3 year average.\newline  
3. Country name – the abbreviated name of the relevant country.\newline  
4. Ladder score – the WHR provides a number of the final happiness score of the current country (from 1.3 to 7.9).\newline  
5. upperwhisker – the upper boundary of the whisker score (from 1.43 to 7.91).\newline  
6. lowerwhisker – the lower boundary of the whisker score (from 1.3 to 7.78).\newline  
7. Log GDP per capita – a normalized version of the country's GDP index in comparison to the rest of the world (normalized in range from 0 to 2.2).\newline  
8. Social support – a normalized version of the country's social support index in comparison to the rest of the world (normalized in range from 0 to 1.8).\newline  
9. Healthy life expectancy – a normalized version of the country's healthy life expectancy index in comparison to the rest of the world (normalized in range from 0 to 1.13).\newline  
10. Freedom to make life choices – a normalized version of the country's freedom to make life choices index in comparison to the rest of the world (normalized in range from 0 to 1.018).\newline  
11. Generosity – a normalized version of the country's generosity index in comparison to the rest of the world (normalized in range from 0 to 0.57).\newline  
12. Perceptions of corruption – a normalized version of the country's perceptions of corruption index in comparison to the rest of the world (normalized in range from 0 to 0.587).\newline  
14. Top 20 – a binary variable indicating whether the country is in the top 20 in that particular year (the outcome variable).\newline

Rows: 1,969\newline Columns: 14\newline \$ Year <dbl> 2024, 2023, 2022, 2021, 2020, 2019…\newline \$ Rank <dbl> 1, 143, 137, 146, 150, 153, 154,…\newline \$ Country_name <chr> "Finland", "Afghanistan", "Afghanistan", …\newline \$ Ladder_score <dbl> 7.7360, 1.7210, 1.8590, 2.4040, …\newline \$ upperwhisker <dbl> 7.810000, 1.775000, 1.923000, 2.…\newline \$ lowerwhisker <dbl> 7.662000, 1.667000, 1.795000, 2.…\newline \$ Log_GDP <dbl> 1.7490000, 0.6280000, 0.6450000,…\newline \$ Social_support <dbl> 1.7830000, 0.0000000, 0.0000000,…\newline \$ Life_expectancy <dbl> 0.8240000, 0.2420000, 0.0870000,…\newline \$ Freedom_of_choices <dbl> 0.9860000, 0.0000000, 0.0000000,…\newline \$ Generosity <dbl> 0.1100000, 0.0910000, 0.0930000,…\newline \$ Corruption <dbl> 0.502000000, 0.088000000, 0.0590…\newline \$ Dystopia_and_residual <dbl> 1.782000, 0.672000, 0.976000, 1.…\newline \$ Top_20 <lgl> TRUE, FALSE, FALSE, FALSE, FALSE…\newline
